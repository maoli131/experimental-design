{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370789f1-a728-41c8-982b-efbb0ef3aa91",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experimental Design on Multilayer Preceptron\n",
    "\n",
    "This notebook performs the analysis and experimental design on the Multilayer Preceptron trained on Fashion-MNIST dataset. \n",
    "It automatically records the train/test accuracy of MLP trained with different portions of datasets (10% ... 100%). \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- Model: a multilayer preceptron that consists of four linear layer with ReLU activation functions. \n",
    "```\n",
    "MLP(\n",
    "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  (linear_relu_stack): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=200, out_features=128, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
    "    (5): ReLU()\n",
    "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "```\n",
    "- Dataset: Fashion MNIST dataset, with 60,000 training samples and 10,000 testing samples. Each sample is a 28 x 28 grayscale image, that is out of the 10 classes: {\"T-Shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"}\n",
    "\n",
    "To run this notebook, we can simply edit the `config` dictionary and run the pipeline. Model performances, metrics and stats will be automatically logged. Adjust the `sls` parameter in `config` to use different portion of training dataset.\n",
    "\n",
    "**Note**: This notebook uses `wandb` to automatically logs the model performance. Following the official tutorial of `wandb`, this notebook integrates `wandb` with a multilayer preceptron model trained on Fashion-MNIST dataset. Before running this notebook, make sure\n",
    "```bash\n",
    "pip install wandb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5410fa56-b306-4f7f-8eeb-9a25eeb98654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "\n",
    "# from tqdm.notebook import tqdm # progress bar\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device Configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3a118b9-61c7-419a-84bb-4bbb7540351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fcfe39-130a-400e-984d-14d8f86864eb",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "We must login to wandb account, in order to record our training on the platform's dashborad. Make sure to install wandb library in our virtual conda environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b16c1cb6-7c0e-4d3f-98c3-7774894a5684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dfd76-8c30-41d9-a1f3-1cb7a3611434",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Data Loading and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf031035-ef1f-4b1a-951b-ffa71fbb7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data with config (slice)\n",
    "def get_data(sls=5, train=True):\n",
    "    \n",
    "    full_dataset = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "    # equiv to slicing iwth [::slice]\n",
    "    sub_dataset = Subset(full_dataset, indices=range(0, int(sls * len(full_dataset)), 1))\n",
    "    \n",
    "    return sub_dataset\n",
    "\n",
    "# Make the dataloader with config (dataset, batch_size\n",
    "def make_loader(dataset, batch_size):\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea3c73e9-98b7-4148-89cb-ab234868e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model: a simple multilayer preceptron\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, kernels, classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, kernels[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(kernels[0], kernels[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(kernels[1], kernels[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(kernels[2], classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef19a84-30ca-4a33-8471-df001c214058",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Training Logic\n",
    "\n",
    "`wandb.watch` will log the gradients and the parameters of your model, every `log_freq` steps of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "250de5a2-92f2-45c1-867c-73cda9d88aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, config):\n",
    "    \n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    \n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        for _, (images, labels) in enumerate(loader):\n",
    "\n",
    "            loss = train_batch(images, labels, model, optimizer, criterion)\n",
    "            example_ct +=  len(images)\n",
    "            batch_ct += 1\n",
    "\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct + 1) % 100) == 0:\n",
    "                train_log(loss, example_ct, epoch)\n",
    "\n",
    "def train_batch(images, labels, model, optimizer, criterion):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Forward pass \n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830e783-cf52-460a-a79d-c397c5dfd495",
   "metadata": {},
   "source": [
    "`wandb.log` records the reported metrics to their server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5b3fcf0-aa02-42d9-b89c-b2df66930304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64fd46-8b17-40ae-936b-aa54269518b7",
   "metadata": {},
   "source": [
    "## Define Testing Logic\n",
    "\n",
    "Once the model is done training, we want to test it: run it against some fresh data from production.\n",
    "\n",
    "We can save the model's architecture and final parameters to disk. We'll `export` our model in the\n",
    "[Open Neural Network eXchange (ONNX) format](https://onnx.ai/).\n",
    "\n",
    "Passing that filename to `wandb.save` ensures that the model parameters are saved to W&B's servers: no more losing track of which `.h5` or `.pb` corresponds to which training runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1689ebf9-54fd-47ff-8deb-2382d27248ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader: #TODO\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy of the model on the {total} \" +\n",
    "              f\"train images: {100 * correct / total}%\")\n",
    "        \n",
    "        wandb.log({\"train_accuracy\": correct / total})\n",
    "        train_acc.append(correct / total)\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, images, \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5b072-4ee7-4f3d-bfa4-b536b06f950d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the experiment and pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7683e-17cc-4496-b719-706c85f913b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make\n",
    "\n",
    "To ensure the values we chose and logged are always the ones that get used\n",
    "in our model, we use the `wandb.config` copy of your object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24e6405e-cc87-4b22-b8e5-30c764db93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data, TODO: both data are training 6000 data\n",
    "    train, test = get_data(train=True, sls = config.sls), get_data(train=True, sls= config.sls)\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
    "\n",
    "    # Make the model\n",
    "    model = MLP(config.kernels, config.classes).to(device)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, train_loader, test_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2c5c1-3a59-4153-b6ac-ee9a6a18c59c",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "The overall pipeline is structured as the following:\n",
    "1. we first `make` a model, plus associated data and optimizer, then\n",
    "2. we `train` the model accordingly and finally\n",
    "3. `test` it to see how training went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e77c575b-60ec-40f5-ab8b-fceaa2912636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"mlp-final\", config=hyperparameters):\n",
    "      \n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, test_loader, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, train_loader, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7cb1b2-48b0-45f0-a0f4-e4f2684fe270",
   "metadata": {},
   "source": [
    "### Run the Model \n",
    "\n",
    "- Config: Hyperparameters and metadata for our model is stored in a dictionary `config`.\n",
    "- Pipeline: build, train and analyze the model with pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e6acff6-600f-4631-a28b-3963d5748946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/maoli131/mlp-final/runs/3exxdtkh\" target=\"_blank\">lyric-sky-1</a></strong> to <a href=\"https://wandb.ai/maoli131/mlp-final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Loss after 06320 examples: 0.937\n",
      "Loss after 12704 examples: 0.408\n",
      "Loss after 19088 examples: 0.460\n",
      "Loss after 25472 examples: 0.361\n",
      "Loss after 31856 examples: 0.543\n",
      "Loss after 38240 examples: 0.480\n",
      "Loss after 44624 examples: 0.347\n",
      "Loss after 51008 examples: 0.419\n",
      "Loss after 57392 examples: 0.364\n",
      "Accuracy of the model on the 6000 train images: 89.33333333333333%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 59784... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.77MB of 0.77MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>loss</td><td>█▂▂▁▃▃▁▂▁</td></tr><tr><td>train_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.36394</td></tr><tr><td>train_accuracy</td><td>0.89333</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lyric-sky-1</strong>: <a href=\"https://wandb.ai/maoli131/mlp-final/runs/3exxdtkh\" target=\"_blank\">https://wandb.ai/maoli131/mlp-final/runs/3exxdtkh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211209_181548-3exxdtkh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/maoli131/mlp-final/runs/3w1vshq3\" target=\"_blank\">expert-energy-2</a></strong> to <a href=\"https://wandb.ai/maoli131/mlp-final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Loss after 06336 examples: 0.709\n",
      "Loss after 12736 examples: 0.529\n",
      "Loss after 19088 examples: 0.538\n",
      "Loss after 25488 examples: 0.414\n",
      "Loss after 31888 examples: 0.398\n",
      "Loss after 38240 examples: 0.511\n",
      "Loss after 44640 examples: 0.376\n",
      "Loss after 51040 examples: 0.415\n",
      "Loss after 57392 examples: 0.463\n",
      "Loss after 63792 examples: 0.355\n",
      "Loss after 70192 examples: 0.278\n",
      "Loss after 76544 examples: 0.305\n",
      "Loss after 82944 examples: 0.320\n",
      "Loss after 89344 examples: 0.432\n",
      "Loss after 95696 examples: 0.386\n",
      "Loss after 102096 examples: 0.266\n",
      "Loss after 108448 examples: 0.264\n",
      "Loss after 114848 examples: 0.230\n",
      "Loss after 121248 examples: 0.272\n",
      "Loss after 127600 examples: 0.276\n",
      "Loss after 134000 examples: 0.346\n",
      "Loss after 140400 examples: 0.266\n",
      "Loss after 146752 examples: 0.249\n",
      "Loss after 153152 examples: 0.284\n",
      "Loss after 159552 examples: 0.293\n",
      "Loss after 165904 examples: 0.481\n",
      "Loss after 172304 examples: 0.288\n",
      "Loss after 178704 examples: 0.211\n",
      "Accuracy of the model on the 18000 train images: 89.70555555555555%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 61397... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.77MB of 0.77MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>loss</td><td>█▅▆▄▄▅▃▄▅▃▂▂▃▄▃▂▂▁▂▂▃▂▂▂▂▅▂▁</td></tr><tr><td>train_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.21095</td></tr><tr><td>train_accuracy</td><td>0.89706</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">expert-energy-2</strong>: <a href=\"https://wandb.ai/maoli131/mlp-final/runs/3w1vshq3\" target=\"_blank\">https://wandb.ai/maoli131/mlp-final/runs/3w1vshq3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211209_181607-3w1vshq3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/maoli131/mlp-final/runs/17hdmh53\" target=\"_blank\">vague-rain-3</a></strong> to <a href=\"https://wandb.ai/maoli131/mlp-final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Loss after 06336 examples: 0.785\n",
      "Loss after 12736 examples: 0.550\n",
      "Loss after 19136 examples: 0.743\n",
      "Loss after 25536 examples: 0.539\n",
      "Loss after 31920 examples: 0.408\n",
      "Loss after 38320 examples: 0.534\n",
      "Loss after 44720 examples: 0.420\n",
      "Loss after 51120 examples: 0.439\n",
      "Loss after 57520 examples: 0.357\n",
      "Loss after 63904 examples: 0.698\n",
      "Loss after 70304 examples: 0.388\n",
      "Loss after 76704 examples: 0.311\n",
      "Loss after 83104 examples: 0.396\n",
      "Loss after 89504 examples: 0.486\n",
      "Loss after 95888 examples: 0.387\n",
      "Loss after 102288 examples: 0.275\n",
      "Loss after 108688 examples: 0.213\n",
      "Loss after 115088 examples: 0.192\n",
      "Loss after 121472 examples: 0.231\n",
      "Loss after 127872 examples: 0.272\n",
      "Loss after 134272 examples: 0.266\n",
      "Loss after 140672 examples: 0.236\n",
      "Loss after 147072 examples: 0.170\n",
      "Loss after 153456 examples: 0.299\n",
      "Loss after 159856 examples: 0.507\n",
      "Loss after 166256 examples: 0.199\n",
      "Loss after 172656 examples: 0.342\n",
      "Loss after 179056 examples: 0.306\n",
      "Loss after 185440 examples: 0.393\n",
      "Loss after 191840 examples: 0.237\n",
      "Loss after 198240 examples: 0.319\n",
      "Loss after 204640 examples: 0.425\n",
      "Loss after 211024 examples: 0.265\n",
      "Loss after 217424 examples: 0.265\n",
      "Loss after 223824 examples: 0.240\n",
      "Loss after 230224 examples: 0.281\n",
      "Loss after 236624 examples: 0.099\n",
      "Loss after 243008 examples: 0.408\n",
      "Loss after 249408 examples: 0.200\n",
      "Loss after 255808 examples: 0.186\n",
      "Loss after 262208 examples: 0.427\n",
      "Loss after 268608 examples: 0.238\n",
      "Loss after 274992 examples: 0.276\n",
      "Loss after 281392 examples: 0.196\n",
      "Loss after 287792 examples: 0.271\n",
      "Loss after 294192 examples: 0.224\n",
      "Accuracy of the model on the 30000 train images: 90.28666666666666%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/m/maoli/.conda/envs/tutorial/lib/python3.8/site-packages/wandb/wandb_torch.py:222: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if flat.shape == torch.Size([0]):\n",
      "/global/homes/m/maoli/.conda/envs/tutorial/lib/python3.8/site-packages/wandb/wandb_torch.py:225: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  tmin = flat.min().item()\n",
      "/global/homes/m/maoli/.conda/envs/tutorial/lib/python3.8/site-packages/wandb/wandb_torch.py:226: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  tmax = flat.max().item()\n",
      "/global/homes/m/maoli/.conda/envs/tutorial/lib/python3.8/site-packages/wandb/wandb_torch.py:261: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  {name: wandb.Histogram(np_histogram=(tensor.tolist(), bins.tolist()))}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2177... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.77MB of 0.77MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆█▅▄▅▄▄▇▄▃▄▅▄▂▂▂▃▃▂▃▅▂▃▃▄▂▄▃▃▂▃▁▂▂▄▂▃▂▃</td></tr><tr><td>train_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.22377</td></tr><tr><td>train_accuracy</td><td>0.90287</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vague-rain-3</strong>: <a href=\"https://wandb.ai/maoli131/mlp-final/runs/17hdmh53\" target=\"_blank\">https://wandb.ai/maoli131/mlp-final/runs/17hdmh53</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211209_181654-17hdmh53/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/maoli131/mlp-final/runs/2zqlfwmo\" target=\"_blank\">bright-cloud-4</a></strong> to <a href=\"https://wandb.ai/maoli131/mlp-final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Loss after 06336 examples: 0.719\n",
      "Loss after 12736 examples: 0.567\n",
      "Loss after 19136 examples: 0.745\n",
      "Loss after 25536 examples: 0.493\n",
      "Loss after 31936 examples: 0.396\n",
      "Loss after 38336 examples: 0.393\n",
      "Loss after 44688 examples: 0.482\n",
      "Loss after 51088 examples: 0.393\n",
      "Loss after 57488 examples: 0.543\n",
      "Loss after 63888 examples: 0.457\n",
      "Loss after 70288 examples: 0.577\n",
      "Loss after 76688 examples: 0.346\n",
      "Loss after 83088 examples: 0.398\n",
      "Loss after 89440 examples: 0.520\n",
      "Loss after 95840 examples: 0.373\n",
      "Loss after 102240 examples: 0.388\n",
      "Loss after 108640 examples: 0.472\n",
      "Loss after 115040 examples: 0.324\n",
      "Loss after 121440 examples: 0.286\n",
      "Loss after 127792 examples: 0.261\n",
      "Loss after 134192 examples: 0.283\n",
      "Loss after 140592 examples: 0.203\n",
      "Loss after 146992 examples: 0.254\n",
      "Loss after 153392 examples: 0.460\n",
      "Loss after 159792 examples: 0.456\n",
      "Loss after 166192 examples: 0.455\n",
      "Loss after 172544 examples: 0.248\n",
      "Loss after 178944 examples: 0.316\n",
      "Loss after 185344 examples: 0.393\n",
      "Loss after 191744 examples: 0.317\n",
      "Loss after 198144 examples: 0.553\n",
      "Loss after 204544 examples: 0.386\n",
      "Loss after 210896 examples: 0.270\n",
      "Loss after 217296 examples: 0.212\n",
      "Loss after 223696 examples: 0.283\n",
      "Loss after 230096 examples: 0.387\n",
      "Loss after 236496 examples: 0.174\n",
      "Loss after 242896 examples: 0.201\n",
      "Loss after 249296 examples: 0.238\n",
      "Loss after 255648 examples: 0.286\n",
      "Loss after 262048 examples: 0.203\n",
      "Loss after 268448 examples: 0.255\n",
      "Loss after 274848 examples: 0.191\n",
      "Loss after 281248 examples: 0.196\n",
      "Loss after 287648 examples: 0.260\n",
      "Loss after 294000 examples: 0.276\n",
      "Loss after 300400 examples: 0.191\n",
      "Loss after 306800 examples: 0.293\n",
      "Loss after 313200 examples: 0.225\n",
      "Loss after 319600 examples: 0.347\n",
      "Loss after 326000 examples: 0.140\n",
      "Loss after 332400 examples: 0.201\n",
      "Loss after 338752 examples: 0.169\n",
      "Loss after 345152 examples: 0.227\n",
      "Loss after 351552 examples: 0.237\n",
      "Loss after 357952 examples: 0.151\n",
      "Loss after 364352 examples: 0.242\n",
      "Loss after 370752 examples: 0.174\n",
      "Loss after 377152 examples: 0.108\n",
      "Loss after 383504 examples: 0.312\n",
      "Loss after 389904 examples: 0.377\n",
      "Loss after 396304 examples: 0.328\n",
      "Loss after 402704 examples: 0.152\n",
      "Loss after 409104 examples: 0.128\n",
      "Loss after 415504 examples: 0.184\n",
      "Accuracy of the model on the 42000 train images: 90.71904761904761%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7426... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.77MB of 0.77MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>loss</td><td>█▆▅▄▅▆▅▃▆▄▅▃▂▂▂▅▂▃▃▄▃▃▁▂▃▂▂▂▃▃▄▁▁▂▁▁▃▄▁▂</td></tr><tr><td>train_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.18366</td></tr><tr><td>train_accuracy</td><td>0.90719</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bright-cloud-4</strong>: <a href=\"https://wandb.ai/maoli131/mlp-final/runs/2zqlfwmo\" target=\"_blank\">https://wandb.ai/maoli131/mlp-final/runs/2zqlfwmo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211209_181810-2zqlfwmo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/maoli131/mlp-final/runs/1m8uskxw\" target=\"_blank\">wandering-grass-5</a></strong> to <a href=\"https://wandb.ai/maoli131/mlp-final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Loss after 06336 examples: 0.692\n",
      "Loss after 12736 examples: 0.550\n",
      "Loss after 19136 examples: 0.731\n",
      "Loss after 25536 examples: 0.512\n",
      "Loss after 31936 examples: 0.408\n",
      "Loss after 38336 examples: 0.380\n",
      "Loss after 44736 examples: 0.302\n",
      "Loss after 51136 examples: 0.392\n",
      "Loss after 57536 examples: 0.388\n",
      "Loss after 63904 examples: 0.621\n",
      "Loss after 70304 examples: 0.414\n",
      "Loss after 76704 examples: 0.334\n",
      "Loss after 83104 examples: 0.403\n",
      "Loss after 89504 examples: 0.544\n",
      "Loss after 95904 examples: 0.286\n",
      "Loss after 102304 examples: 0.424\n",
      "Loss after 108704 examples: 0.420\n",
      "Loss after 115104 examples: 0.214\n",
      "Loss after 121472 examples: 0.278\n",
      "Loss after 127872 examples: 0.211\n",
      "Loss after 134272 examples: 0.305\n",
      "Loss after 140672 examples: 0.325\n",
      "Loss after 147072 examples: 0.222\n",
      "Loss after 153472 examples: 0.610\n",
      "Loss after 159872 examples: 0.235\n",
      "Loss after 166272 examples: 0.314\n",
      "Loss after 172672 examples: 0.363\n",
      "Loss after 179072 examples: 0.225\n",
      "Loss after 185440 examples: 0.467\n",
      "Loss after 191840 examples: 0.354\n",
      "Loss after 198240 examples: 0.376\n",
      "Loss after 204640 examples: 0.409\n",
      "Loss after 211040 examples: 0.393\n",
      "Loss after 217440 examples: 0.216\n",
      "Loss after 223840 examples: 0.463\n",
      "Loss after 230240 examples: 0.340\n",
      "Loss after 236640 examples: 0.380\n",
      "Loss after 243008 examples: 0.403\n",
      "Loss after 249408 examples: 0.331\n",
      "Loss after 255808 examples: 0.243\n",
      "Loss after 262208 examples: 0.411\n",
      "Loss after 268608 examples: 0.205\n",
      "Loss after 275008 examples: 0.288\n",
      "Loss after 281408 examples: 0.314\n",
      "Loss after 287808 examples: 0.232\n",
      "Loss after 294208 examples: 0.332\n",
      "Loss after 300576 examples: 0.296\n",
      "Loss after 306976 examples: 0.290\n",
      "Loss after 313376 examples: 0.341\n",
      "Loss after 319776 examples: 0.134\n",
      "Loss after 326176 examples: 0.208\n",
      "Loss after 332576 examples: 0.152\n",
      "Loss after 338976 examples: 0.169\n",
      "Loss after 345376 examples: 0.259\n",
      "Loss after 351776 examples: 0.386\n",
      "Loss after 358176 examples: 0.228\n",
      "Loss after 364544 examples: 0.217\n",
      "Loss after 370944 examples: 0.322\n",
      "Loss after 377344 examples: 0.315\n",
      "Loss after 383744 examples: 0.255\n",
      "Loss after 390144 examples: 0.382\n",
      "Loss after 396544 examples: 0.295\n",
      "Loss after 402944 examples: 0.284\n",
      "Loss after 409344 examples: 0.292\n",
      "Loss after 415744 examples: 0.390\n",
      "Loss after 422112 examples: 0.256\n",
      "Loss after 428512 examples: 0.121\n",
      "Loss after 434912 examples: 0.215\n",
      "Loss after 441312 examples: 0.293\n",
      "Loss after 447712 examples: 0.308\n",
      "Loss after 454112 examples: 0.343\n",
      "Loss after 460512 examples: 0.203\n",
      "Loss after 466912 examples: 0.286\n",
      "Loss after 473312 examples: 0.352\n",
      "Loss after 479712 examples: 0.408\n",
      "Loss after 486080 examples: 0.203\n",
      "Loss after 492480 examples: 0.330\n",
      "Loss after 498880 examples: 0.272\n",
      "Loss after 505280 examples: 0.159\n",
      "Loss after 511680 examples: 0.297\n",
      "Loss after 518080 examples: 0.205\n",
      "Loss after 524480 examples: 0.153\n",
      "Loss after 530880 examples: 0.299\n",
      "Loss after 537280 examples: 0.108\n",
      "Loss after 543648 examples: 0.294\n",
      "Loss after 550048 examples: 0.177\n",
      "Loss after 556448 examples: 0.222\n",
      "Loss after 562848 examples: 0.103\n",
      "Loss after 569248 examples: 0.123\n",
      "Loss after 575648 examples: 0.165\n",
      "Loss after 582048 examples: 0.517\n",
      "Loss after 588448 examples: 0.103\n",
      "Loss after 594848 examples: 0.237\n",
      "Accuracy of the model on the 60000 train images: 91.18833333333333%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13397... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.77MB of 0.77MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>loss</td><td>██▄▄▇▄▃▅▃▃▇▃▅▄▂▄▄▄▃▂▃▁▂▄▂▃▃▃▁▃▄▄▂▃▂▃▃▁▂▂</td></tr><tr><td>train_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.23662</td></tr><tr><td>train_accuracy</td><td>0.91188</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wandering-grass-5</strong>: <a href=\"https://wandb.ai/maoli131/mlp-final/runs/1m8uskxw\" target=\"_blank\">https://wandb.ai/maoli131/mlp-final/runs/1m8uskxw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211209_181952-1m8uskxw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# configurations\n",
    "config = dict(\n",
    "    epochs=10,\n",
    "    classes=10,\n",
    "    kernels=[200, 128, 128],\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    sls = 0.1,\n",
    "    dataset=\"Fashion-MNIST\",\n",
    "    architecture=\"MLP\"\n",
    ")\n",
    "\n",
    "# Build, train and analyze the model with the pipeline\n",
    "portions = [0.1, 0.3, 0.5, 0.7, 1]\n",
    "for sls in portions:\n",
    "    config['sls'] = sls\n",
    "    model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5201670e-8516-4be1-9840-35045dcd90aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.title(\"Accuracy vs. % Training Data Used\")\n",
    "plt.xlabel(\"% Training Data Used\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "sns.scatterplot(x=portions, y=train_acc)\n",
    "sns.lineplot(x=portions, y=train_acc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7359178-032d-4e61-9d41-c7d6de03920b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tutorial",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
